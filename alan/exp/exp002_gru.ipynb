{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acc6aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from contextlib import contextmanager\n",
    "import logging\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, average_precision_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, KFold,GroupKFold,StratifiedGroupKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder\n",
    "from torch.nn import TransformerDecoder\n",
    "from torch.nn import LayerNorm\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.optim import lr_scheduler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "%matplotlib inline\n",
    "import logging\n",
    "pd.set_option('display.max_columns', 300)\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57d87879",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "exp = \"002\"\n",
    "if not os.path.exists(f\"../out/exp/exp{exp}\"):\n",
    "    os.makedirs(f\"../out/exp/exp{exp}\")\n",
    "    os.makedirs(f\"../out/exp/exp{exp}/exp{exp}_model\")\n",
    "logger_path = f\"../out/exp/exp{exp}/exp_{exp}.txt\"\n",
    "model_path =f\"../out/exp/exp{exp}/exp{exp}_model/exp{exp}.pth\"\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "file_handler = logging.FileHandler(logger_path)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "LOGGER.addHandler(file_handler)\n",
    "\n",
    "# config\n",
    "seed = 0\n",
    "shuffle = True\n",
    "n_splits = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model config\n",
    "batch_size = 32\n",
    "n_epochs = 5\n",
    "lr = 1e-4\n",
    "weight_decay = 0.05\n",
    "num_warmup_steps = 10\n",
    "\n",
    "id_path = f\"../out/fe/fe002/id_list.npy\"\n",
    "player_path = f\"../out/fe/fe002/player_list.npy\"\n",
    "feature_arr_path = f\"../out/fe/fe002/feature_arr.npy\"\n",
    "target_arr_path = f\"../out/fe/fe002/target_arr.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aacce733",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_arr = np.load(feature_arr_path)\n",
    "feature_arr = feature_arr.astype(np.float32)\n",
    "target_arr = np.load(target_arr_path)\n",
    "target_arr = target_arr.astype(np.float32)\n",
    "id_list = np.load(id_path)\n",
    "player_list = np.load(player_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5c4de57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwingDataset(Dataset):\n",
    "    def __init__(self, X, \n",
    "                 train = True, y = None):\n",
    "        self.X = X\n",
    "        self.train = train\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        else:\n",
    "            return self.X[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89900a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwingGRU(nn.Module):\n",
    "    def __init__(\n",
    "        self, dropout=0.2,\n",
    "        input_dim = 24,\n",
    "        hidden_dim = 64,\n",
    "        model_dim = 128,\n",
    "        out_size = 11\n",
    "        ):\n",
    "        super(SwingGRU, self).__init__()\n",
    "        self.numerical_linear  = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim)\n",
    "            )\n",
    "        \n",
    "        self.rnn = nn.GRU(hidden_dim, model_dim,\n",
    "                            num_layers = 2, \n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "                \n",
    "        self.linear_out  = nn.Sequential(\n",
    "                nn.Linear(model_dim * 2, \n",
    "                          model_dim),\n",
    "                nn.LayerNorm(model_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(model_dim, \n",
    "                          out_size),\n",
    "                nn.Sigmoid(),\n",
    "        )\n",
    "        self._reinitialize()\n",
    "        \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'rnn' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "    \n",
    "    def forward(self, numerical_array):\n",
    "        \n",
    "        numerical_embedding = self.numerical_linear(numerical_array)\n",
    "        output,_ = self.rnn(numerical_embedding)\n",
    "        output = self.linear_out(output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create sinusoidal positional encoding\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register as buffer (not a parameter but should be saved and loaded with the model)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "class EncoderOnlyClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=24, n_enc=3, nhead=8, d_model=64, max_seq_len=1000):\n",
    "        super().__init__()\n",
    "        # Initialize Transformer model\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(input_dim, d_model),\n",
    "        )\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_enc)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, 11),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Project input to d_model dimension\n",
    "        x = self.input_proj(src)  # -> (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Pass through transformer encoder\n",
    "        memory = self.encoder(x)\n",
    "        \n",
    "        # Use the last time-step from encoder output\n",
    "        last = memory[:, -1, :]  # shape: (batch_size, d_model)\n",
    "        logits = self.classifier(last)  # shape: (batch_size, 11)\n",
    "        \n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a293411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 11])\n"
     ]
    }
   ],
   "source": [
    "model = EncoderOnlyClassifier()\n",
    "y = model(torch.randn(2, 1000, 24))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb20012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = target_arr[:, -4:]\n",
    "target_labels = np.argmax(target_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61f76aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = StratifiedGroupKFold(n_splits=5,shuffle=True,random_state = seed)\n",
    "iterator = gkf.split(feature_arr, y = target_labels, groups= player_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c6c380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = SwingDataset(feature_arr, train=True, y = target_arr)\n",
    "train_loader = DataLoader(train_ds, batch_size=2, pin_memory=True)\n",
    "criterion = nn.BCELoss()\n",
    "X, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8ba0ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SwingGRU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7066a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(X)\n",
    "loss = criterion(y, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cba59bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5833, 0.6206, 0.6263, 0.5055],\n",
       "        [0.8018, 0.6430, 0.4366, 0.4115]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:, -4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "631fbb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8010761000000001"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.62496989 - 0.5 + 0.67610621"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f19fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(p: torch.Tensor):\n",
    "    if p.requires_grad:\n",
    "        return p.detach().cpu().numpy()\n",
    "    else:\n",
    "        return p.cpu().numpy()\n",
    "\n",
    "def metric_report(y_batch, out_batch):\n",
    "    cut = [0, 2, 4, 7, 11]\n",
    "    classes = ['gender', 'hand', 'year', 'level']\n",
    "    for start, end, cls in zip(cut, cut[1:], classes):\n",
    "        micro_roc_score = roc_auc_score(y_batch[:, start:end], out_batch[:, start:end], average='micro', multi_class='ovr')\n",
    "        macro_roc_score = roc_auc_score(y_batch[:, start:end], out_batch[:, start:end], average='macro', multi_class='ovr')\n",
    "        micro_presicion_score = average_precision_score(y_batch[:, start:end], out_batch[:, start:end], average='micro')\n",
    "        macro_presicion_score = average_precision_score(y_batch[:, start:end], out_batch[:, start:end], average='macro')\n",
    "        \n",
    "        print(f\"{cls} micro roc: {micro_roc_score:.4f}, macro roc: {macro_roc_score:.4f}, micro presci: {micro_presicion_score:.4f}, macro presci: {macro_presicion_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42b5abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #AUC SCORE: 0.792(gender) + 0.998(hold) + 0.660(years) + 0.822(levels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3613d694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fold:0, train size: 6882, val size: 1520\n",
      "class weights: [0.07866459 0.30622187 0.5473914  0.06772215]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a43bd8cbb34174a225aa07c1802bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/216 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 86\u001b[0m\n\u001b[1;32m     83\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     84\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 86\u001b[0m train_out_batch\u001b[38;5;241m.\u001b[39mappend(\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     87\u001b[0m train_y_batch\u001b[38;5;241m.\u001b[39mappend(to_numpy(y))\n\u001b[1;32m     88\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m, in \u001b[0;36mto_numpy\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_numpy\u001b[39m(p: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m----> 3\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m p\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 20\n",
    "lr = 1e-4\n",
    "weight_decay = 0.05\n",
    "num_warmup_steps = 10\n",
    "main_loss_weight = 0.8\n",
    "class_start = 7\n",
    "class_end = 11\n",
    "\n",
    "# train levels first\n",
    "# target_labels = target_arr[:, class_start:class_end]\n",
    "# target_labels = np.argmax(target_labels, axis=1)\n",
    "\n",
    "# gkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state = seed)\n",
    "# iterator = gkf.split(feature_arr, y = target_labels, groups = player_list)\n",
    "gf = GroupKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "iterator = gf.split(feature_arr, target_arr, groups=player_list)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(iterator):\n",
    "    print(f\"start fold:{fold}, train size: {len(train_idx)}, val size: {len(val_idx)}\")\n",
    "        \n",
    "    train_feature = feature_arr[train_idx]\n",
    "    train_target = target_arr[train_idx]\n",
    "    \n",
    "    train_weights = 1 / np.sum(train_target, axis=0)\n",
    "    level_weights = train_weights[class_start:class_end] / np.sum(train_weights[class_start:class_end])\n",
    "    print(f\"class weights: {level_weights}\")\n",
    "    \n",
    "    val_feature = feature_arr[val_idx]\n",
    "    val_target = target_arr[val_idx]\n",
    "\n",
    "    train_ds = SwingDataset(train_feature, train=True, y = train_target)\n",
    "    val_ds = SwingDataset(val_feature, train=True, y = val_target)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, pin_memory=True, num_workers=4)\n",
    "    \n",
    "    # model = SwingGRU()\n",
    "    model = EncoderOnlyClassifier()\n",
    "    model = model.to(device)\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                        lr=lr,\n",
    "                        weight_decay=weight_decay,\n",
    "                        )\n",
    "    num_train_optimization_steps = int(len(train_loader) * n_epochs)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=num_warmup_steps,\n",
    "                                                num_training_steps=num_train_optimization_steps)\n",
    "\n",
    "    main_criterion = nn.BCELoss(weight=torch.tensor(level_weights).to(device))\n",
    "    aux_criterion = nn.BCELoss()\n",
    "    # criterion = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(n_epochs):                \n",
    "        train_out_batch = []\n",
    "        val_out_batch = []\n",
    "        train_y_batch = []\n",
    "        val_y_batch = []\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        model.train() \n",
    "        pbar = tqdm(train_loader, total=len(train_loader), leave = False)\n",
    "        for d in pbar:\n",
    "            X, y = d\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model(X)\n",
    "            aux_loss = aux_criterion(out[:, :7], y[:, :7])\n",
    "            main_loss = main_criterion(out[:, 7:], y[:, 7:])\n",
    "            loss = (1 - main_loss_weight) * aux_loss + main_loss_weight * main_loss\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_out_batch.append(to_numpy(out))\n",
    "            train_y_batch.append(to_numpy(y))\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_out_batch = np.concatenate(train_out_batch)\n",
    "        train_y_batch = np.concatenate(train_y_batch)\n",
    "        \n",
    "        print(f\"Train loss: {train_loss/len(train_loader):.4f}\")\n",
    "        # metric_report(train_y_batch, train_out_batch)\n",
    "        # break\n",
    "        \n",
    "        model.eval()\n",
    "        pbar = tqdm(val_loader, total=len(val_loader), leave = False)\n",
    "        with torch.no_grad():\n",
    "            for d in pbar:\n",
    "                X, y = d\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                \n",
    "                out = model(X)\n",
    "                aux_loss = aux_criterion(out[:, :7], y[:, :7])\n",
    "                main_loss = main_criterion(out[:, 7:], y[:, 7:])\n",
    "                loss = (1 - main_loss_weight) * aux_loss + main_loss_weight * main_loss\n",
    "\n",
    "                val_out_batch.append(to_numpy(out))\n",
    "                val_y_batch.append(to_numpy(y))\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_out_batch = np.concatenate(val_out_batch)\n",
    "        val_y_batch = np.concatenate(val_y_batch)\n",
    "        \n",
    "        print(f\"Val loss: {val_loss/len(val_loader):.4f}\")\n",
    "        metric_report(val_y_batch, val_out_batch)\n",
    "        \n",
    "    break\n",
    "                        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
