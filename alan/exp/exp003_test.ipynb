{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acc6aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from contextlib import contextmanager\n",
    "import logging\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, average_precision_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, KFold,GroupKFold,StratifiedGroupKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder\n",
    "from torch.nn import TransformerDecoder\n",
    "from torch.nn import LayerNorm\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.optim import lr_scheduler, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "from types import SimpleNamespace\n",
    "%matplotlib inline\n",
    "import logging\n",
    "pd.set_option('display.max_columns', 300)\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57d87879",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "exp = \"003\"\n",
    "if not os.path.exists(f\"../out/exp/exp{exp}\"):\n",
    "    os.makedirs(f\"../out/exp/exp{exp}\")\n",
    "    os.makedirs(f\"../out/exp/exp{exp}/exp{exp}_model\")\n",
    "logger_path = f\"../out/exp/exp{exp}/exp_{exp}.txt\"\n",
    "model_path =f\"../out/exp/exp{exp}/exp{exp}_model/exp{exp}.pth\"\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "file_handler = logging.FileHandler(logger_path)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "LOGGER.addHandler(file_handler)\n",
    "\n",
    "# config\n",
    "seed = 0\n",
    "shuffle = True\n",
    "n_splits = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model config\n",
    "batch_size = 32\n",
    "n_epochs = 5\n",
    "lr = 1e-4\n",
    "weight_decay = 0.05\n",
    "num_warmup_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aacce733",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_path = f\"../out/fe/fe004/train_feature.npy\"\n",
    "train_target_path = f\"../out/fe/fe004/train_target.npy\"\n",
    "val_feature_path = f\"../out/fe/fe004/val_feature.npy\"\n",
    "val_target_path = f\"../out/fe/fe004/val_target.npy\"\n",
    "\n",
    "\n",
    "train_feature = np.load(train_feature_path)\n",
    "train_target = np.load(train_target_path)\n",
    "val_feature = np.load(val_feature_path)\n",
    "val_target = np.load(val_target_path)\n",
    "\n",
    "train_feature = train_feature.astype(np.float32)\n",
    "val_feature = val_feature.astype(np.float32)\n",
    "train_target = train_target.astype(np.float32)\n",
    "val_target = val_target.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5c4de57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwingDataset(Dataset):\n",
    "    def __init__(self, X, \n",
    "                 train = True, y = None):\n",
    "        self.X = X\n",
    "        self.train = train\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        else:\n",
    "            return self.X[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89900a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwingGRU(nn.Module):\n",
    "    def __init__(\n",
    "        self, dropout=0.2,\n",
    "        input_dim = 24,\n",
    "        hidden_dim = 64,\n",
    "        model_dim = 128,\n",
    "        out_size = 11\n",
    "        ):\n",
    "        super(SwingGRU, self).__init__()\n",
    "        self.numerical_linear  = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim)\n",
    "            )\n",
    "        \n",
    "        self.rnn = nn.GRU(hidden_dim, model_dim,\n",
    "                            num_layers = 2, \n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "                \n",
    "        self.linear_out  = nn.Sequential(\n",
    "                nn.Linear(model_dim * 2, \n",
    "                          model_dim),\n",
    "                nn.LayerNorm(model_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(model_dim, \n",
    "                          out_size),\n",
    "                # nn.Sigmoid(),\n",
    "        )\n",
    "        self._reinitialize()\n",
    "        \n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'rnn' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "    \n",
    "    def forward(self, numerical_array):\n",
    "        \n",
    "        numerical_embedding = self.numerical_linear(numerical_array)\n",
    "        output,_ = self.rnn(numerical_embedding)\n",
    "        # last = output[:, -1, :]\n",
    "        last = torch.mean(output, dim=1)\n",
    "        output = self.linear_out(last)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create sinusoidal positional encoding\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register as buffer (not a parameter but should be saved and loaded with the model)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "class EncoderOnlyClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=24, n_enc=2, nhead=8, d_model=64, max_seq_len=1000):\n",
    "        super().__init__()\n",
    "        # Initialize Transformer model\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(input_dim, d_model),\n",
    "        )\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_enc)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, 11),\n",
    "            # nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Project input to d_model dimension\n",
    "        x = self.input_proj(src)  # -> (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        # x = self.pos_encoder(x)\n",
    "        \n",
    "        # Pass through transformer encoder\n",
    "        memory = self.encoder(x)\n",
    "        \n",
    "        # Use the last time-step from encoder output\n",
    "        # last = torch.mean(memory, dim=1)\n",
    "        last = memory[:, -1, :]  # shape: (batch_size, d_model)\n",
    "        logits = self.classifier(last)  # shape: (batch_size, 11)\n",
    "        \n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f19fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(p: torch.Tensor):\n",
    "    if p.requires_grad:\n",
    "        return p.detach().cpu().numpy()\n",
    "    else:\n",
    "        return p.cpu().numpy()\n",
    "\n",
    "def metric_report(y_batch, out_batch):\n",
    "    cut = [0, 2, 4, 7, 11]\n",
    "    classes = ['gender', 'hand', 'year', 'level']\n",
    "    for start, end, cls in zip(cut, cut[1:], classes):\n",
    "        micro_roc_score = roc_auc_score(y_batch[:, start:end], out_batch[:, start:end], average='micro', multi_class='ovr')\n",
    "        macro_roc_score = roc_auc_score(y_batch[:, start:end], out_batch[:, start:end], average='macro', multi_class='ovr')\n",
    "        micro_presicion_score = average_precision_score(y_batch[:, start:end], out_batch[:, start:end], average='micro')\n",
    "        macro_presicion_score = average_precision_score(y_batch[:, start:end], out_batch[:, start:end], average='macro')\n",
    "        \n",
    "        print(f\"{cls} micro roc: {micro_roc_score:.4f}, macro roc: {macro_roc_score:.4f}, micro presci: {micro_presicion_score:.4f}, macro presci: {macro_presicion_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42b5abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #AUC SCORE: 0.792(gender) + 0.998(hold) + 0.660(years) + 0.822(levels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62988dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(loss_tpye, class_weights):\n",
    "    criterions = []\n",
    "    cut = [0, 2, 4, 7, 11]\n",
    "    classes = ['gender', 'hand', 'year', 'level']\n",
    "\n",
    "    for start, end in zip(cut, cut[1:]):\n",
    "        partial_weights = torch.tensor(class_weights[start:end]).to(device)\n",
    "        if loss_tpye == 'CE':\n",
    "            criterion = nn.CrossEntropyLoss(weight=partial_weights)\n",
    "        elif loss_tpye == 'BCE':\n",
    "            criterion = nn.BCEWithLogitsLoss(weight=partial_weights)\n",
    "        else:\n",
    "            raise ValueError(f\"no such loss {loss_tpye}\")\n",
    "        criterions.append(criterion)\n",
    "    \n",
    "    return criterions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fd2fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace(\n",
    "    batch_size = 32,\n",
    "    n_epochs = 10,\n",
    "    lr = 1e-5,\n",
    "    weight_decay = 0.05,\n",
    "    main_loss_weight = 0.6,\n",
    "    loss_type = 'BCE',\n",
    "    model_type = 'gru',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21d1e9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender weights: [0.1936848 0.8063152]\n",
      "hand weights: [0.13666478 0.8633352 ]\n",
      "year weights: [0.43455723 0.3082604  0.2571824 ]\n",
      "level weights: [0.08001771 0.31609336 0.53850025 0.06538874]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d901bcc5f1646989ef091479a50f9dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5408 Aux loss: 0.7710 Main loss: 0.1380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1768511c5cbd4b4db7175a2312f4cf00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5481 Aux loss: 0.7890 Main loss: 0.1265\n",
      "gender micro roc: 0.9290, macro roc: 0.4963, micro presci: 0.9167, macro presci: 0.5001\n",
      "hand micro roc: 0.6158, macro roc: 0.5559, micro presci: 0.5721, macro presci: 0.5305\n",
      "year micro roc: 0.6291, macro roc: nan, micro presci: 0.4222, macro presci: 0.3311\n",
      "level micro roc: 0.6351, macro roc: 0.5175, micro presci: 0.3393, macro presci: 0.2554\n",
      "✨ best Val loss at epoch 0, loss 0.1265\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce92aa5ec144df1b9281779fb1d39cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4700 Aux loss: 0.6801 Main loss: 0.1023\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34f51879b634e939a279e8664306304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5372 Aux loss: 0.7768 Main loss: 0.1179\n",
      "gender micro roc: 0.9374, macro roc: 0.5327, micro presci: 0.9247, macro presci: 0.5050\n",
      "hand micro roc: 0.6366, macro roc: 0.5944, micro presci: 0.5917, macro presci: 0.5596\n",
      "year micro roc: 0.6147, macro roc: nan, micro presci: 0.3963, macro presci: 0.3336\n",
      "level micro roc: 0.6617, macro roc: 0.5184, micro presci: 0.3662, macro presci: 0.2575\n",
      "✨ best Val loss at epoch 1, loss 0.1179\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90869e8d472241fbad4e9c48c009223b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4526 Aux loss: 0.6582 Main loss: 0.0927\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c731b71a44a4ceb97e46a9dcdea9159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5310 Aux loss: 0.7680 Main loss: 0.1163\n",
      "gender micro roc: 0.9416, macro roc: 0.5619, micro presci: 0.9294, macro presci: 0.5097\n",
      "hand micro roc: 0.6530, macro roc: 0.6280, micro presci: 0.6056, macro presci: 0.5876\n",
      "year micro roc: 0.6134, macro roc: nan, micro presci: 0.3914, macro presci: 0.3364\n",
      "level micro roc: 0.6688, macro roc: 0.5201, micro presci: 0.3742, macro presci: 0.2597\n",
      "✨ best Val loss at epoch 2, loss 0.1163\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3e25b056844c10aab993ac192f18c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4421 Aux loss: 0.6439 Main loss: 0.0891\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97bc66e2ece642fc80f12ede8fa14436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5229 Aux loss: 0.7555 Main loss: 0.1159\n",
      "gender micro roc: 0.9446, macro roc: 0.5851, micro presci: 0.9333, macro presci: 0.5139\n",
      "hand micro roc: 0.6686, macro roc: 0.6600, micro presci: 0.6192, macro presci: 0.6178\n",
      "year micro roc: 0.6123, macro roc: nan, micro presci: 0.3920, macro presci: 0.3397\n",
      "level micro roc: 0.6722, macro roc: 0.5220, micro presci: 0.3780, macro presci: 0.2619\n",
      "✨ best Val loss at epoch 3, loss 0.1159\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81d08b2429d4c16bea885ba9a6c36b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4323 Aux loss: 0.6293 Main loss: 0.0876\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff7f56d9d124690ac5fb28a47c720b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5169 Aux loss: 0.7460 Main loss: 0.1160\n",
      "gender micro roc: 0.9475, macro roc: 0.6093, micro presci: 0.9379, macro presci: 0.5187\n",
      "hand micro roc: 0.6825, macro roc: 0.6887, micro presci: 0.6329, macro presci: 0.6491\n",
      "year micro roc: 0.6066, macro roc: nan, micro presci: 0.3903, macro presci: 0.3437\n",
      "level micro roc: 0.6746, macro roc: 0.5247, micro presci: 0.3814, macro presci: 0.2651\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc5332cb5f44169bfcf7762fdb7d5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4215 Aux loss: 0.6127 Main loss: 0.0870\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fac05a89ac425eab2fe4e8e81fa836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5085 Aux loss: 0.7329 Main loss: 0.1160\n",
      "gender micro roc: 0.9502, macro roc: 0.6322, micro presci: 0.9423, macro presci: 0.5235\n",
      "hand micro roc: 0.6954, macro roc: 0.7151, micro presci: 0.6470, macro presci: 0.6820\n",
      "year micro roc: 0.6107, macro roc: nan, micro presci: 0.3986, macro presci: 0.3484\n",
      "level micro roc: 0.6764, macro roc: 0.5292, micro presci: 0.3841, macro presci: 0.2693\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21922d925f043e6a8532b60cb47e0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4107 Aux loss: 0.5956 Main loss: 0.0869\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2012bb94f4ec49d595f84fe17ea81b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4977 Aux loss: 0.7160 Main loss: 0.1157\n",
      "gender micro roc: 0.9523, macro roc: 0.6498, micro presci: 0.9459, macro presci: 0.5271\n",
      "hand micro roc: 0.7070, macro roc: 0.7382, micro presci: 0.6606, macro presci: 0.7157\n",
      "year micro roc: 0.6196, macro roc: nan, micro presci: 0.4126, macro presci: 0.3527\n",
      "level micro roc: 0.6782, macro roc: 0.5348, micro presci: 0.3859, macro presci: 0.2733\n",
      "✨ best Val loss at epoch 6, loss 0.1157\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d6f7f534d54f799ec37fa9c569c6d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4024 Aux loss: 0.5825 Main loss: 0.0871\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573df1d7f7d546beab0f99bc5d748ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4933 Aux loss: 0.7092 Main loss: 0.1156\n",
      "gender micro roc: 0.9541, macro roc: 0.6653, micro presci: 0.9491, macro presci: 0.5305\n",
      "hand micro roc: 0.7151, macro roc: 0.7535, micro presci: 0.6696, macro presci: 0.7365\n",
      "year micro roc: 0.6194, macro roc: nan, micro presci: 0.4168, macro presci: 0.3556\n",
      "level micro roc: 0.6798, macro roc: 0.5392, micro presci: 0.3877, macro presci: 0.2767\n",
      "✨ best Val loss at epoch 7, loss 0.1156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae216c0fe4c4e39963d4b451b3fa05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3959 Aux loss: 0.5723 Main loss: 0.0870\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18870559805e49889b41b642bc0a5707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4902 Aux loss: 0.7043 Main loss: 0.1155\n",
      "gender micro roc: 0.9550, macro roc: 0.6740, micro presci: 0.9509, macro presci: 0.5323\n",
      "hand micro roc: 0.7203, macro roc: 0.7629, micro presci: 0.6756, macro presci: 0.7486\n",
      "year micro roc: 0.6180, macro roc: nan, micro presci: 0.4183, macro presci: 0.3573\n",
      "level micro roc: 0.6809, macro roc: 0.5420, micro presci: 0.3890, macro presci: 0.2790\n",
      "✨ best Val loss at epoch 8, loss 0.1155\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa0d448550140e7baac0d69cd0210e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3929 Aux loss: 0.5677 Main loss: 0.0870\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927e175465d64c94a244e30e6ed17b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4899 Aux loss: 0.7038 Main loss: 0.1155\n",
      "gender micro roc: 0.9553, macro roc: 0.6770, micro presci: 0.9515, macro presci: 0.5330\n",
      "hand micro roc: 0.7219, macro roc: 0.7656, micro presci: 0.6773, macro presci: 0.7517\n",
      "year micro roc: 0.6177, macro roc: nan, micro presci: 0.4189, macro presci: 0.3579\n",
      "level micro roc: 0.6813, macro roc: 0.5429, micro presci: 0.3896, macro presci: 0.2798\n",
      "✨ best Val loss at epoch 9, loss 0.1155\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 10\n",
    "lr = 1e-5\n",
    "weight_decay = 0.05\n",
    "num_warmup_steps = 10\n",
    "main_loss_weight = 4/11\n",
    "loss_type = 'BCE'\n",
    "\n",
    "set_seed(42)\n",
    "cut = [0, 2, 4, 7, 11]\n",
    "classes = ['gender', 'hand', 'year', 'level']\n",
    "\n",
    "train_weights = 1 / np.sum(train_target, axis=0)\n",
    "final_weights = np.zeros((11,))\n",
    "for start, end, cls in zip(cut, cut[1:], classes):\n",
    "    class_weights = train_weights[start:end] / np.sum(train_weights[start:end])\n",
    "    final_weights[start:end] = class_weights\n",
    "    print(f\"{cls} weights: {class_weights}\")\n",
    "\n",
    "train_ds = SwingDataset(train_feature, train=True, y = train_target)\n",
    "val_ds = SwingDataset(val_feature, train=True, y = val_target)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, pin_memory=True, num_workers=4)\n",
    "\n",
    "# model = SwingGRU()\n",
    "model = EncoderOnlyClassifier()\n",
    "model = model.to(device)\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                    lr=lr,\n",
    "                    weight_decay=weight_decay,\n",
    "                    )\n",
    "\n",
    "num_train_optimization_steps = int(len(train_loader) * n_epochs)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=num_warmup_steps,\n",
    "                                            num_training_steps=num_train_optimization_steps)\n",
    "\n",
    "criterions = get_loss(loss_type, final_weights)\n",
    "\n",
    "best_loss = 9999.9\n",
    "early_stop_cnt = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_out_batch = []\n",
    "    val_out_batch = []\n",
    "    train_y_batch = []\n",
    "    val_y_batch = []\n",
    "    train_loss = 0.0\n",
    "    train_main_loss = 0.0\n",
    "    train_aux_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    val_main_loss = 0.0\n",
    "    val_aux_loss = 0.0\n",
    "    \n",
    "    model.train() \n",
    "    pbar = tqdm(train_loader, total=len(train_loader), leave = False)\n",
    "    for d in pbar:\n",
    "        X, y = d\n",
    "        b, t, c = X.size()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(X)\n",
    "        \n",
    "        aux_loss = 0\n",
    "        main_loss = 0\n",
    "        aux_loss += criterions[0](out[:, :2], y[:, :2])\n",
    "        aux_loss += criterions[1](out[:, 2:4], y[:, 2:4])\n",
    "        aux_loss += criterions[2](out[:, 4:7], y[:, 4:7])\n",
    "        main_loss += criterions[3](out[:, 7:], y[:, 7:])\n",
    "        loss = (1 - main_loss_weight) * aux_loss + main_loss_weight * main_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if loss_type == 'CE':\n",
    "            train_out_batch.append(to_numpy(F.softmax(out)))\n",
    "        elif loss_type == 'BCE':\n",
    "            train_out_batch.append(to_numpy(F.sigmoid(out)))\n",
    "\n",
    "        train_y_batch.append(to_numpy(y))\n",
    "        train_loss += loss.item()\n",
    "        train_main_loss += main_loss.item()\n",
    "        train_aux_loss += aux_loss.item()\n",
    "    \n",
    "    train_out_batch = np.concatenate(train_out_batch)\n",
    "    train_y_batch = np.concatenate(train_y_batch)\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    train_main_loss = train_main_loss/len(train_loader)\n",
    "    train_aux_loss = train_aux_loss/len(train_loader)\n",
    "    print(f\"Train loss: {train_loss:.4f} Aux loss: {train_aux_loss:.4f} Main loss: {train_main_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    pbar = tqdm(val_loader, total=len(val_loader), leave = False)\n",
    "    with torch.no_grad():\n",
    "        for d in pbar:\n",
    "            X, y = d\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            out = model(X)\n",
    "            aux_loss = 0\n",
    "            main_loss = 0\n",
    "            aux_loss += criterions[0](out[:, :2], y[:, :2])\n",
    "            aux_loss += criterions[1](out[:, 2:4], y[:, 2:4])\n",
    "            aux_loss += criterions[2](out[:, 4:7], y[:, 4:7])\n",
    "            main_loss += criterions[3](out[:, 7:], y[:, 7:])\n",
    "            loss = (1 - main_loss_weight) * aux_loss + main_loss_weight * main_loss\n",
    "\n",
    "            if loss_type == 'CE':\n",
    "                val_out_batch.append(to_numpy(F.softmax(out)))\n",
    "            elif loss_type == 'BCE':\n",
    "                val_out_batch.append(to_numpy(F.sigmoid(out)))\n",
    "\n",
    "            val_y_batch.append(to_numpy(y))\n",
    "            val_loss += loss.item()\n",
    "            val_main_loss += main_loss.item()\n",
    "            val_aux_loss += aux_loss.item()\n",
    "\n",
    "\n",
    "    val_out_batch = np.concatenate(val_out_batch)\n",
    "    val_y_batch = np.concatenate(val_y_batch)\n",
    "\n",
    "    val_loss = val_loss/len(val_loader)\n",
    "    val_main_loss = val_main_loss/len(val_loader)\n",
    "    val_aux_loss = val_aux_loss/len(val_loader)\n",
    "    print(f\"Val loss: {val_loss:.4f} Aux loss: {val_aux_loss:.4f} Main loss: {val_main_loss:.4f}\")\n",
    "    metric_report(val_y_batch, val_out_batch)\n",
    "    \n",
    "    if val_main_loss < best_loss:\n",
    "        best_loss = val_main_loss\n",
    "        print(f\"✨ best Val loss at epoch {epoch}, loss {val_main_loss:.4f}\")\n",
    "        early_stop_cnt = 0\n",
    "    else:\n",
    "        early_stop_cnt += 1\n",
    "    \n",
    "    if early_stop_cnt == 3:\n",
    "        print(f\"No imporvement for {early_stop_cnt} Epochs, stopping\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da866339",
   "metadata": {},
   "source": [
    "gru BCE main weight 0.6\n",
    "gender micro roc: 0.8772, macro roc: 0.5145, micro presci: 0.8666, macro presci: 0.5027\n",
    "hand micro roc: 0.9836, macro roc: 0.9998, micro presci: 0.9860, macro presci: 0.9999\n",
    "year micro roc: 0.7048, macro roc: nan, micro presci: 0.5178, macro presci: 0.3901\n",
    "level micro roc: 0.7452, macro roc: 0.6534, micro presci: 0.5440, macro presci: 0.4111\n",
    "✨ best Val loss at epoch 9, loss 0.1065\n",
    "\n",
    "gru CE main weight 0.6\n",
    "gender micro roc: 0.7335, macro roc: 0.7485, micro presci: 0.7244, macro presci: 0.5415\n",
    "hand micro roc: 0.9855, macro roc: 0.9858, micro presci: 0.9852, macro presci: 0.9852\n",
    "year micro roc: 0.6026, macro roc: nan, micro presci: 0.4718, macro presci: 0.3886\n",
    "level micro roc: 0.5973, macro roc: 0.6532, micro presci: 0.3167, macro presci: 0.4098\n",
    "✨ best Val loss at epoch 0, loss 0.2248\n",
    "\n",
    "encoder CE main weight 0.6\n",
    "gender micro roc: 0.4246, macro roc: 0.5286, micro presci: 0.4889, macro presci: 0.5041\n",
    "hand micro roc: 0.5058, macro roc: 0.4927, micro presci: 0.5479, macro presci: 0.5466\n",
    "year micro roc: 0.5587, macro roc: nan, micro presci: 0.3985, macro presci: 0.3384\n",
    "level micro roc: 0.5122, macro roc: 0.4933, micro presci: 0.2539, macro presci: 0.2658\n",
    "✨ best Val loss at epoch 0, loss 0.2379\n",
    "\n",
    "encoder BCE main weight 0.6\n",
    "gender micro roc: 0.9370, macro roc: 0.5172, micro presci: 0.9355, macro presci: 0.5067\n",
    "hand micro roc: 0.5816, macro roc: 0.4799, micro presci: 0.5629, macro presci: 0.5166\n",
    "year micro roc: 0.6991, macro roc: nan, micro presci: 0.4950, macro presci: 0.3312\n",
    "level micro roc: 0.6710, macro roc: 0.4723, micro presci: 0.3606, macro presci: 0.2458\n",
    "✨ best Val loss at epoch 0, loss 0.1142\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
